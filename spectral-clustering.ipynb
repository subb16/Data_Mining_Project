{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74693,"databundleVersionId":8362605,"sourceType":"competition"},{"sourceId":8372392,"sourceType":"datasetVersion","datasetId":4977344}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom scipy.sparse.linalg import eigsh\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport random\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-05-10T16:11:34.829251Z","iopub.execute_input":"2024-05-10T16:11:34.829749Z","iopub.status.idle":"2024-05-10T16:11:42.413994Z","shell.execute_reply.started":"2024-05-10T16:11:34.829707Z","shell.execute_reply":"2024-05-10T16:11:42.412563Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"##### SEED DATA","metadata":{}},{"cell_type":"code","source":"seed = pd.read_excel('/kaggle/input/da324dataminingproject2/seed.xlsx', sheet_name='in')\nseed.columns = range(len(seed.columns))\n\n# making the column headings as a row\nall_seeds = pd.DataFrame(seed.columns).T\nall_seeds = pd.concat([all_seeds, seed], axis=0)\nall_seeds = all_seeds.reset_index(drop = True)\nall_seeds.columns = [\"First\", \"Second\", \"Third\"]","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:11:42.416140Z","iopub.execute_input":"2024-05-10T16:11:42.417286Z","iopub.status.idle":"2024-05-10T16:11:43.129174Z","shell.execute_reply.started":"2024-05-10T16:11:42.417242Z","shell.execute_reply":"2024-05-10T16:11:43.127489Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"##### ATTRIBUTES DATA","metadata":{}},{"cell_type":"code","source":"attributes = pd.read_excel('/kaggle/input/da324dataminingproject2/attributes.xlsx', sheet_name='in')\n\n# standardize the attributes\nattribute_standardized = (attributes - attributes.mean()) / attributes.std()\n\n# apply pca\npca = PCA(n_components=0.9)  \nattributes_pca = pca.fit_transform(attribute_standardized)\nattributes_pca = pd.DataFrame(data=attributes_pca, columns=[f\"PC{i+1}\" for i in range(attributes_pca.shape[1])])\n\n# normalize the attributes\nattribute_normalized = attributes_pca.apply(lambda x: x / np.linalg.norm(x))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:11:43.130851Z","iopub.execute_input":"2024-05-10T16:11:43.131386Z","iopub.status.idle":"2024-05-10T16:12:10.453308Z","shell.execute_reply.started":"2024-05-10T16:11:43.131353Z","shell.execute_reply":"2024-05-10T16:12:10.451319Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"##### ADJACENCY DATA","metadata":{}},{"cell_type":"code","source":"adjacency = pd.read_csv(\"/kaggle/input/da324dataminingproject2/adjacency.csv\")\n\n# USE THE BELOW COMMENTED CODE FOR THE OLD ADJACENCY DATA\n\n# adjacency = pd.read_excel('/kaggle/input/da324dataminingproject2/adjacency.xlsx', sheet_name='in')\n# def clean_data(row):\n#     row = row.split(\"\\n\")\n#     nodes = []\n#     for node in row:\n#         if node == '  :\\t:':\n#             continue\n#         nodes.append(int(node[6:-5]))  \n#     return nodes\n# adjacency[\"nodes\"] = adjacency.iloc[:, 0].apply(clean_data)\n\n# adjacecny_matrix = np.zeros((11952, 11952))\n# for node1, row in adjacency.iterrows():\n#     for node2 in row[\"nodes\"]:\n#         adjacecny_matrix[node1, node2] = 1","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:12:10.457960Z","iopub.execute_input":"2024-05-10T16:12:10.459085Z","iopub.status.idle":"2024-05-10T16:12:50.590226Z","shell.execute_reply.started":"2024-05-10T16:12:10.459005Z","shell.execute_reply":"2024-05-10T16:12:50.588890Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# calculating the laplacian matrix\nadjacecny_matrix = adjacency.to_numpy()\ndegree_matrix = np.diag(np.sum(adjacecny_matrix, axis=1))\nlaplacian_matrix = degree_matrix - adjacecny_matrix\nlaplacian_matrix = laplacian_matrix.astype(float)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:12:50.591758Z","iopub.execute_input":"2024-05-10T16:12:50.593426Z","iopub.status.idle":"2024-05-10T16:12:53.238422Z","shell.execute_reply.started":"2024-05-10T16:12:50.593368Z","shell.execute_reply":"2024-05-10T16:12:53.237218Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# calculating the 10 smallest eigenvectors of laplacian\neigenvalues, eigenvectors = eigsh(laplacian_matrix, k=10, which='SM')\neigenvectors = pd.DataFrame(eigenvectors, columns=[f\"col_{i+1}\" for i in range(eigenvectors.shape[1])])","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:12:53.239577Z","iopub.execute_input":"2024-05-10T16:12:53.239929Z","iopub.status.idle":"2024-05-10T16:17:30.859819Z","shell.execute_reply.started":"2024-05-10T16:12:53.239897Z","shell.execute_reply":"2024-05-10T16:17:30.857888Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"##### Concatentating Adjacency and Attributes data to get final embeddings","metadata":{}},{"cell_type":"code","source":"embeddings = pd.concat([attribute_normalized, eigenvectors], axis=1)\nembeddings = (embeddings - embeddings.mean()) / embeddings.std()\npca = PCA(n_components=0.9)  \nfinal_embeddings = pca.fit_transform(embeddings)\nfinal_embeddings = pd.DataFrame(data=final_embeddings, columns=[f\"PC{i+1}\" for i in range(final_embeddings.shape[1])])","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:17:30.865148Z","iopub.execute_input":"2024-05-10T16:17:30.865729Z","iopub.status.idle":"2024-05-10T16:17:31.070146Z","shell.execute_reply.started":"2024-05-10T16:17:30.865681Z","shell.execute_reply":"2024-05-10T16:17:31.068265Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# seperating the first 10952 rows for training\ntrain_embeddings = final_embeddings.iloc[:10952, :]","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:17:31.079449Z","iopub.execute_input":"2024-05-10T16:17:31.085693Z","iopub.status.idle":"2024-05-10T16:17:31.097598Z","shell.execute_reply.started":"2024-05-10T16:17:31.085587Z","shell.execute_reply":"2024-05-10T16:17:31.095815Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"##### APPLYING KMEANS CLUSTERING","metadata":{}},{"cell_type":"code","source":"# calculating initial centroids from given seeds' centroid\ncentroids = np.zeros((10, final_embeddings.shape[1]))\nfor index, row in all_seeds.iterrows():\n    centroids[index] = (final_embeddings.iloc[row['First'], :] +  final_embeddings.iloc[row['Second'], :] +  final_embeddings.iloc[row['Third'], :])/3","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:17:31.106735Z","iopub.execute_input":"2024-05-10T16:17:31.112666Z","iopub.status.idle":"2024-05-10T16:17:31.145413Z","shell.execute_reply.started":"2024-05-10T16:17:31.112558Z","shell.execute_reply":"2024-05-10T16:17:31.142927Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# applying kmeans\nkmeans = KMeans(n_clusters=10, init=centroids, n_init=1, random_state=0)\nlabels = kmeans.fit_predict(train_embeddings)\ntrain_embeddings['cluster'] = labels","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:17:31.162206Z","iopub.execute_input":"2024-05-10T16:17:31.163546Z","iopub.status.idle":"2024-05-10T16:17:31.442205Z","shell.execute_reply.started":"2024-05-10T16:17:31.163471Z","shell.execute_reply":"2024-05-10T16:17:31.441264Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"##### TRAINING A NEURAL NETWORK ON THE DATA","metadata":{}},{"cell_type":"code","source":"# dataloader class\nclass CustomDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long) if y is not None else None\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.X[idx], self.y[idx]\n        else:\n            return self.X[idx]","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:17:31.443452Z","iopub.execute_input":"2024-05-10T16:17:31.444215Z","iopub.status.idle":"2024-05-10T16:17:31.452054Z","shell.execute_reply.started":"2024-05-10T16:17:31.444180Z","shell.execute_reply":"2024-05-10T16:17:31.451085Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"X_train = np.array(train_embeddings.iloc[:, :-1])\ny_train = np.array(train_embeddings.iloc[:, -1])\nX_test = np.array(final_embeddings.iloc[-1000:, :])\n\ntrain_dataset = CustomDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ntest_dataset = CustomDataset(X_test)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:17:31.453603Z","iopub.execute_input":"2024-05-10T16:17:31.454373Z","iopub.status.idle":"2024-05-10T16:17:31.506005Z","shell.execute_reply.started":"2024-05-10T16:17:31.454340Z","shell.execute_reply":"2024-05-10T16:17:31.505110Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# model class\nclass Custom_model(torch.nn.Module):\n    def __init__(self):\n        super(Custom_model, self).__init__()\n        self.fc1 = torch.nn.Linear(final_embeddings.shape[1], 128)\n        self.relu = torch.nn.ReLU()\n        self.fc2 = torch.nn.Linear(128, 10)  \n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:17:31.507575Z","iopub.execute_input":"2024-05-10T16:17:31.508206Z","iopub.status.idle":"2024-05-10T16:17:31.514915Z","shell.execute_reply.started":"2024-05-10T16:17:31.508175Z","shell.execute_reply":"2024-05-10T16:17:31.513823Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# defining the loss function and the optimizer\nmodel = Custom_model()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:17:31.517057Z","iopub.execute_input":"2024-05-10T16:17:31.517437Z","iopub.status.idle":"2024-05-10T16:17:34.411638Z","shell.execute_reply.started":"2024-05-10T16:17:31.517395Z","shell.execute_reply":"2024-05-10T16:17:34.410008Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"num_epochs = 30\nfor epoch in range(num_epochs):\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    if epoch%5 == 0:\n        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:17:34.413542Z","iopub.execute_input":"2024-05-10T16:17:34.414217Z","iopub.status.idle":"2024-05-10T16:17:45.616561Z","shell.execute_reply.started":"2024-05-10T16:17:34.414177Z","shell.execute_reply":"2024-05-10T16:17:45.615218Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.5939496159553528\nEpoch 6, Loss: 0.09112799912691116\nEpoch 11, Loss: 0.06277771294116974\nEpoch 16, Loss: 0.00908169150352478\nEpoch 21, Loss: 0.02355409786105156\nEpoch 26, Loss: 0.0020169666968286037\n","output_type":"stream"}]},{"cell_type":"markdown","source":"##### PREDICTING THE LABELS OF REMAINING DATA","metadata":{}},{"cell_type":"code","source":"model.eval()\npredictions = []\n\nwith torch.no_grad():\n    for inputs in test_loader:\n        outputs = model(inputs)\n        _,predicted = torch.max(outputs.data, 1)\n        predictions.extend(np.array(predicted.squeeze()))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:17:45.619116Z","iopub.execute_input":"2024-05-10T16:17:45.619681Z","iopub.status.idle":"2024-05-10T16:17:45.640674Z","shell.execute_reply.started":"2024-05-10T16:17:45.619634Z","shell.execute_reply":"2024-05-10T16:17:45.639253Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"final_embeddings[\"cluster\"] = [0]*11952\nfinal_embeddings.iloc[:10952, -1] = train_embeddings[\"cluster\"]\nfinal_embeddings.iloc[10952:, -1] = predictions","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:17:45.642865Z","iopub.execute_input":"2024-05-10T16:17:45.643545Z","iopub.status.idle":"2024-05-10T16:17:45.659346Z","shell.execute_reply.started":"2024-05-10T16:17:45.643497Z","shell.execute_reply":"2024-05-10T16:17:45.657894Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"##### FINAL SUBMISSION FILE","metadata":{}},{"cell_type":"code","source":"final_embeddings.reset_index(inplace=True)\nsubmission_labels = final_embeddings[['index', 'cluster']].rename(columns={'index': 'ID', 'cluster': 'LABEL'})\nsubmission_labels.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T16:17:45.660787Z","iopub.execute_input":"2024-05-10T16:17:45.661304Z","iopub.status.idle":"2024-05-10T16:17:45.694790Z","shell.execute_reply.started":"2024-05-10T16:17:45.661258Z","shell.execute_reply":"2024-05-10T16:17:45.693261Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}